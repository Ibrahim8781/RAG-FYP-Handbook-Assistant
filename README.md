# ğŸ“š FYP Handbook RAG Assistant

> An intelligent Retrieval-Augmented Generation (RAG) system for the FAST-NUCES Final Year Project Handbook, powered by Groq LLM and production-ready features.

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.52.2-red.svg)](https://streamlit.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Live Demo:** [https://web-production-5db8b.up.railway.app](https://web-production-5db8b.up.railway.app)

---

## ğŸ¯ Overview

This RAG system allows students to query the FYP Handbook using natural language and receive accurate, conversational answers with page citations. The system features:

- âœ… **Natural Language Understanding** - Ask questions in plain English
- âœ… **Accurate Answers** - Powered by Llama 3.1 (via Groq API)
- âœ… **Source Citations** - Every answer includes page numbers
- âœ… **Production-Ready** - Logging, rate limiting, caching, error handling
- âœ… **Fast Retrieval** - FAISS vector search with sentence transformers
- âœ… **Web Interface** - Clean Streamlit UI with minimal design

---

## ğŸ—ï¸ Architecture

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE                            â”‚
â”‚              Streamlit Web App (app.py)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”œâ”€â–º Input Validation (error_handling.py)
                        â”œâ”€â–º Rate Limiting (rate_limiting.py)
                        â””â”€â–º Logging (logger.py)
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RETRIEVAL       â”‚                    â”‚  GENERATION        â”‚
â”‚  (Vector Search) â”‚                    â”‚  (LLM)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Embedding      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â€¢ Groq API         â”‚
â”‚   Cache          â”‚   Context Chunks   â”‚ â€¢ Llama 3.1 8B     â”‚
â”‚ â€¢ FAISS Index    â”‚                    â”‚ â€¢ Retry Logic      â”‚
â”‚ â€¢ Sentence-BERT  â”‚                    â”‚ â€¢ Rate Limiting    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              Final Answer with Citations
```

### Data Flow

```
1. USER QUERY
   â†“
2. INPUT VALIDATION
   â€¢ Min 3 chars, Max 500 chars
   â€¢ Not empty
   â†“
3. RATE LIMITING
   â€¢ Global: 10 queries/min
   â€¢ Per-User: 20 queries/hour
   â€¢ API: 30 calls/min
   â†“
4. EMBEDDING (with caching)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Cache Check â”‚ â”€â”€â–º HIT â†’ Use cached (1ms)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ MISS
         â†“
   Generate Embedding (100-200ms)
         â”‚
         â†“
   Cache for Future Use
   â†“
5. VECTOR SEARCH (FAISS)
   â€¢ Cosine similarity
   â€¢ Top-K retrieval (K=5)
   â€¢ Threshold: 0.25
   â†“
6. LLM GENERATION (with retry)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Groq API Callâ”‚ â”€â”€â–º Retry on failure (3 attempts)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Backoff: 1s, 2s, 4s
   â†“
7. RESPONSE
   â€¢ Answer with citations
   â€¢ Confidence score
   â€¢ Source chunks
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or higher
- Groq API key ([Get one free](https://console.groq.com/keys))
- 2GB+ RAM
- Git

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/Ibrahim8781/RAG-FYP-Handbook-Assistant.git
cd RAG-FYP-Handbook-Assistant

# 2. Create virtual environment
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env and add your Groq API key:
# GROQ_API_KEY=your_api_key_here

# 5. Ingest the handbook (first time only)
python ingest.py

# 6. Run the application
streamlit run app.py
```

The app will open at **http://localhost:8501**

---

## ğŸ“– Usage

### Web Interface

1. Open http://localhost:8501
2. Type your question in the text box
3. Click "ğŸ” Ask"
4. Get answer with page citations
5. Expand "View Sources" to see retrieved chunks

**Sample Questions:**
- "What are the required chapters of a Development FYP report?"
- "What headings, fonts, and sizes are required?"
- "How do I use 'Ibid.' and 'op. cit.' in citations?"
- "What goes into the Executive Summary?"

### CLI Interface

```bash
python ask.py "What are the FYP requirements?"
```

---

## ğŸ”§ Configuration

### Environment Variables

Create `.env` file:

```bash
# Required: Groq API Key
GROQ_API_KEY=your_api_key_here

# Optional: Environment (auto-detected)
ENV=development  # or production
```

### Customization

Edit `config_env.py`:

```python
class Config:
    # RAG parameters
    CHUNK_SIZE = 300           # Words per chunk
    OVERLAP = 90               # Word overlap
    TOP_K = 5                  # Chunks to retrieve
    SIMILARITY_THRESHOLD = 0.25
    
    # LLM parameters
    GROQ_MODEL = "llama-3.1-8b-instant"
    LLM_TEMPERATURE = 0.3
    LLM_MAX_TOKENS = 1024
    
    # Caching
    CACHE_TTL = 86400          # 24 hours
    CACHE_MAX_SIZE = 1000
```

---

## ğŸ“Š Production Features

### 1. Structured Logging âœ…

**JSON logs in production** for monitoring tools (CloudWatch, Datadog)

```json
{
  "timestamp": "2026-01-13T18:30:45Z",
  "level": "INFO",
  "message": "LLM call successful",
  "model": "llama-3.1-8b-instant",
  "tokens": 512,
  "latency": 1.45
}
```

### 2. Error Handling & Retry Logic âœ…

**Automatic retries** with exponential backoff

- Retry pattern: 0s â†’ 1s â†’ 2s â†’ 4s
- User-friendly error messages
- Input validation (min/max length)

### 3. Rate Limiting âœ…

**Prevents abuse** and protects API costs

- Global: 10 queries/minute
- Per-User: 20 queries/hour
- API: 30 Groq calls/minute

### 4. Embedding Caching âœ…

**90% faster** on cache hits, **50% cost reduction**

- TTL: 24 hours
- LRU eviction (max 1000)
- Persistent across restarts

---

## ğŸš‚ Railway Deployment

### Quick Deploy

1. **Connect GitHub to Railway**
   - Go to [railway.app/dashboard](https://railway.app/dashboard)
   - New Project â†’ Deploy from GitHub
   - Select: RAG-FYP-Handbook-Assistant

2. **Set Environment Variable**
   ```
   GROQ_API_KEY=your_api_key_here
   ```

3. **Deploy**
   - Railway auto-deploys on git push
   - Build time: ~2 minutes
   - Your app: https://your-project.up.railway.app

### Environment Detection

- **Local**: Shows "ğŸ”§ DEVELOPMENT" badge
- **Railway**: Shows "ğŸ­ PRODUCTION" badge
- **Auto-configured**: Based on `RAILWAY_ENVIRONMENT`

---

## ğŸ“ Project Structure

```
RAG-FYP-Handbook-Assistant/
â”‚
â”œâ”€â”€ ğŸ“„ Core Application
â”‚   â”œâ”€â”€ app.py                      # Streamlit web interface
â”‚   â”œâ”€â”€ ask.py                      # CLI interface
â”‚   â”œâ”€â”€ ingest.py                   # Document ingestion
â”‚   â”œâ”€â”€ llm_utils.py                # Groq LLM wrapper
â”‚   â””â”€â”€ config_env.py               # Environment config
â”‚
â”œâ”€â”€ ğŸ—ï¸ Production Infrastructure
â”‚   â”œâ”€â”€ logger.py                   # Structured logging
â”‚   â”œâ”€â”€ error_handling.py           # Retry logic
â”‚   â”œâ”€â”€ rate_limiting.py            # Request throttling
â”‚   â””â”€â”€ caching.py                  # Embedding cache
â”‚
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â”œâ”€â”€ .env.example                # Environment template
â”‚   â”œâ”€â”€ Procfile                    # Railway start command
â”‚   â”œâ”€â”€ railway.json                # Railway config
â”‚   â””â”€â”€ requirements.txt            # Dependencies
â”‚
â”œâ”€â”€ ğŸ“Š Data & Artifacts
â”‚   â”œâ”€â”€ FYP-Handbook-2023.pdf       # Source document
â”‚   â”œâ”€â”€ faiss_index.bin             # Vector index (generated)
â”‚   â””â”€â”€ chunks_metadata.pkl         # Text chunks (generated)
â”‚
â””â”€â”€ ğŸ“š Documentation
    â”œâ”€â”€ README.md                   # This file
    â”œâ”€â”€ PRODUCTION_FEATURES.md      # Feature details
    â””â”€â”€ RAILWAY_DEPLOY.md           # Deployment guide
```

---

## ğŸ› ï¸ API Documentation

### GroqLLM Class

```python
from llm_utils import GroqLLM

# Initialize
llm = GroqLLM(api_key="your_key", model="llama-3.1-8b-instant")

# Generate answer
result = llm.generate_answer(
    question="What are FYP requirements?",
    context="<retrieved chunks>",
    max_tokens=1024,
    temperature=0.3
)

# Response
{
    'success': True,
    'answer': "The FYP requirements include...",
    'tokens_used': {'total': 570},
    'latency': 1.45
}
```

### Caching

```python
from caching import embedding_cache

# Get cached embedding
cached = embedding_cache.get(query)

# Set new embedding
embedding_cache.set(query, embedding)

# Get stats
stats = embedding_cache.get_stats()
```

### Rate Limiting

```python
from rate_limiting import check_rate_limit

allowed, msg = check_rate_limit(user_id)
if not allowed:
    print(f"Rate limited: {msg}")
```

### Logging

```python
from logger import log_query, log_llm_call

log_query("What are FYP requirements?", user_id="user_123")
log_llm_call(model="llama-3.1-8b-instant", tokens=512, latency=1.45, success=True)
```

---

## ğŸ§ª Testing

```bash
# Test individual modules
python logger.py              # Structured logging
python error_handling.py      # Retry logic
python rate_limiting.py       # Rate limits
python caching.py             # Cache operations

# Comprehensive test suite
python test_rag.py 1          # Test local CLI
python test_rag.py 2          # Test Railway
```

---

## ğŸ“ˆ Performance

### Latency

```
Typical Query (cached):     ~1.2 seconds
First Query (uncached):     ~1.4 seconds
Cache Hit:                  ~0.001 seconds (1ms)
FAISS Search:               ~0.010 seconds (10ms)
LLM Generation:             ~1.2 seconds
```

### Cost Analysis

**Per 1000 queries (50% cache hit):**
- Embeddings: $0.05
- LLM (Groq): $0.50
- **Total: $0.55** (vs $0.60 without cache)

### Resource Usage

**Local:**
- RAM: ~300-400 MB
- CPU: 10-20% (idle), 80-100% (query)

**Railway:**
- RAM: ~200-300 MB
- Build time: ~2 minutes
- Cold start: ~10 seconds

---

## ğŸ› Troubleshooting

### "Groq API key not found"

```bash
# Check .env file
cat .env

# Should contain:
GROQ_API_KEY=gsk_your_key_here
```

### "RAG system not initialized"

```bash
# Run ingestion
python ingest.py

# Verify files
ls faiss_index.bin chunks_metadata.pkl
```

### "Rate limit exceeded"

Wait the specified time (shown in error message).

### Slow first query

Normal! Models load on first use (~10-15s). Subsequent queries are fast (~2-3s).

---

## ğŸ¤ Contributing

```bash
# 1. Fork and clone
git clone https://github.com/your-username/RAG-FYP-Handbook-Assistant.git

# 2. Create branch
git checkout -b feature/your-feature

# 3. Make changes and test
python test_rag.py 1

# 4. Commit and push
git commit -m "Add feature"
git push origin feature/your-feature

# 5. Create Pull Request
```

---

## ğŸ“„ License

MIT License - see LICENSE file for details

---

## ğŸ™ Acknowledgments

- **FAST-NUCES** - FYP Handbook source
- **Groq** - Fast LLM inference
- **Streamlit** - Web framework
- **Sentence-Transformers** - Embeddings
- **FAISS** - Vector search
- **Railway** - Deployment platform

---

## ğŸ“ Support

- **Documentation**: [PRODUCTION_FEATURES.md](PRODUCTION_FEATURES.md)
- **Issues**: [GitHub Issues](https://github.com/Ibrahim8781/RAG-FYP-Handbook-Assistant/issues)
- **Repository**: [GitHub](https://github.com/Ibrahim8781/RAG-FYP-Handbook-Assistant)

---

<div align="center">

**Built with â¤ï¸ for FAST-NUCES Students**

[Live Demo](https://web-production-5db8b.up.railway.app) â€¢ [Documentation](PRODUCTION_FEATURES.md) â€¢ [Report Issue](https://github.com/Ibrahim8781/RAG-FYP-Handbook-Assistant/issues)

â­ Star this repo if you found it helpful!

</div>
